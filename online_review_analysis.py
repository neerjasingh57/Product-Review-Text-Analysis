# -*- coding: utf-8 -*-
"""Online Review Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n25Ks-_hg5JzEWE2CSH7S_RwYpf1_ucK
"""

from google.colab import files
import pandas as pd
df = pd.read_excel('Online Reviews Analysis.xlsx')

df.head()

"""###Descriptive Statistics
1. Overall rating
2. Count of each rating
3. Platforms
4. Date posted


"""

#mean rating = 3.76
df.describe()

#Date range
df['Date posted'].unique()

#Platforms
df['Platform'].unique()

#Rating value counts
df['Rating'].value_counts()

plotdata = pd.DataFrame(
    {"Rating": [4, 4, 7, 17, 18]}, 
    index=[1, 2, 3, 4, 5])
# Plot a bar chart
plotdata.plot(kind="bar", alpha = .75)

plt.title("Review Rating Distribution")
plt.xlabel("Rating")
plt.ylabel("Count")

"""###Text Analysis
1. Most common words (by 1 word and 2 words) in review title
2. Most common words (by 1 word and 2 words) in review
3. Clustering of review title 
4. Clustering of review
"""

#pip install -U pip

!pip install pandas configparser spacy wordcloud matplotlib

import os
import pandas as pd
from configparser import ConfigParser
import spacy
from spacy import displacy
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator

import matplotlib.pyplot as plt

!python -m spacy download en_core_web_sm

nlp = spacy.load('en_core_web_sm')

"""###Word Cloud for Title"""

len(df['Review Title'])

texts_pos = []
for i in range(0, 50):
  texts = df['Review Title'][i]
  texts = str(texts)
  doc = nlp(texts, disable = ["parser", "ner"])

  for token in doc:
    texts_pos.append([token.text, token.pos_, token.is_stop])

texts_pos_df = pd.DataFrame(texts_pos, columns = ("word", "pos", "is_stop"))
texts_pos_df_t = texts_pos_df[texts_pos_df.is_stop != True]
texts_words = texts_pos_df_t.word
texts_words_list = texts_words.to_list()
str1 = ' '.join(str(e) for e in texts_words_list)

str1[1:100]

wordcloud = WordCloud(background_color = "white").generate(str1)

plt.imshow(wordcloud, interpolation = 'bilinear')
plt.axis("off")
plt.show()

"""###MOST Common Words in 'Review Title'"""

texts_list = df['Review Title'].tolist()
len(texts_list)

from sklearn.feature_extraction.text import CountVectorizer
!python -m spacy download es_core_news_sm
import spacy

# create an instance of countvectorizer (sklearn)
vect = CountVectorizer()

# loading spaCy for processing / comparison with sklearn functions
nlp = spacy.load('en')

doc_num = 0

raw_docs = []

# We will leverage SpaCy to pull out the lemmenization of the text
# which will extract the 'unique' words across the documents later on.
for docs in texts_list:
  lemm_words = []
  vec_text = nlp(docs)
  doc_num += 1
  for token in vec_text:
    if not (token.is_punct or token.lemma_ == '-PRON-' or token.is_stop):
      lemm_words.append(token.lemma_)
  staging_doc = ' '.join(lemm_words)
  raw_docs.append(staging_doc)

raw_docs[1:5]

vect.fit(raw_docs)

# store the dense matrix
data = vect.transform(raw_docs).toarray()

# store the learned vocabulary
columns = vect.get_feature_names()

# combine the data and columns into a dataframe
pd.DataFrame(data, columns=columns)

"""###n-gram range = 1"""

#Creating TF-IDF

import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vect = TfidfVectorizer(min_df= 2, ngram_range = (1,1))

tfidf_data = tfidf_vect.fit_transform(raw_docs).toarray()
tfidf_columns = tfidf_vect.get_feature_names()
pd.DataFrame(tfidf_data, columns=tfidf_columns)

import matplotlib.pyplot as plt
from collections import Counter

# Exploratory analysis to get a general sense idea of wha

print('Pulling out indivdual words.')
all_words = []
for line in raw_docs:
    words = line.split()
    for word in words:
        all_words.append(word.lower())
print('Unique words extracted: {}'.format(np.unique(all_words)))

word_count = Counter(all_words).most_common(20)
word_count_x = []
word_count_y = []
for word, count in word_count:
    word_count_x.append(word)
    word_count_y.append(count)

plt.figure(figsize=(16,6))
plt.plot(word_count_x, word_count_y, linestyle='-', linewidth=1)
plt.ylabel("Count")
plt.xlabel("Word")
plt.xticks(fontsize='small', rotation=90)
plt.title('Plot of words frequency across Corpus')
plt.show()

"""Out of 20 most common words:

-positive: good, great, like, amazing, awesome, nice

-negative: N/A

-about features: wireless, sound, update, design, use, integration

###Most Common Words in 'Reviews'
"""

texts_list = df['Review'].tolist()
len(texts_list)

# create an instance of countvectorizer (sklearn)
vect = CountVectorizer()

# loading spaCy for processing / comparison with sklearn functions
nlp = spacy.load('en')

texts_pos = []
for i in range(0, 50):
  texts = df['Review'][i]
  texts = str(texts)
  doc = nlp(texts, disable = ["parser", "ner"])

  for token in doc:
    texts_pos.append([token.text, token.pos_, token.is_stop])

doc_num = 0

raw_docs = []

# We will leverage SpaCy to pull out the lemmenization of the text
# which will extract the 'unique' words across the documents later on.
for docs in texts_list:
  lemm_words = []
  vec_text = nlp(docs)
  doc_num += 1
  for token in vec_text:
    if not (token.is_punct or token.lemma_ == '-PRON-' or token.is_stop):
      lemm_words.append(token.lemma_)
  staging_doc = ' '.join(lemm_words)
  raw_docs.append(staging_doc)

raw_docs[1:5]

vect.fit(raw_docs)

# store the dense matrix
data = vect.transform(raw_docs).toarray()

# store the learned vocabulary
columns = vect.get_feature_names()

# combine the data and columns into a dataframe
pd.DataFrame(data, columns=columns)

#Creating TF-IDF

import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vect = TfidfVectorizer(min_df= 2, ngram_range = (1,1))

tfidf_data = tfidf_vect.fit_transform(raw_docs).toarray()
tfidf_columns = tfidf_vect.get_feature_names()
pd.DataFrame(tfidf_data, columns=tfidf_columns)

import matplotlib.pyplot as plt
from collections import Counter

# Exploratory analysis to get a general sense idea of wha

print('Pulling out indivdual words.')
all_words = []
for line in raw_docs:
    words = line.split()
    for word in words:
        all_words.append(word.lower())
print('Unique words extracted: {}'.format(np.unique(all_words)))

word_count = Counter(all_words).most_common(20)
word_count_x = []
word_count_y = []
for word, count in word_count:
    word_count_x.append(word)
    word_count_y.append(count)

plt.figure(figsize=(16,6))
plt.plot(word_count_x, word_count_y, linestyle='-', linewidth=1)
plt.ylabel("Count")
plt.xlabel("Word")
plt.xticks(fontsize='small', rotation=90)
plt.title('Plot of words frequency across Corpus')
plt.show()

"""Out of 20 most common words:

-positive: good, great

-negative: issue

-about features: sound, case, feature, charge, phone, quality, control, assistant, time, use

###Clusters for 'Review Title'
"""

#Sentence Clustering: K-Means

from sklearn.feature_extraction.text import TfidfVectorizer

#Sparse Matrix
vect = TfidfVectorizer(norm='l2', stop_words='english', max_features=1000, ngram_range=(1,1))
#should we alter some of these parameters?

titles_vect = vect.fit_transform(df['Review Title'])
titles_vect.shape

from sklearn.cluster import KMeans, MiniBatchKMeans
import matplotlib.pyplot as plt

# Run the Kmeans algorithm, printing out the SSE to identify the inflection "elbow"
sse = []
list_k = list(range(1, 11))

for k in list_k:
    km = MiniBatchKMeans(n_clusters=k, random_state=3) # use for larger datasets, not as accurate
    #km = KMeans(n_clusters=k)
    km.fit(titles_vect)
    sse.append(km.inertia_)

# Plot sse against k
plt.figure(figsize=(14, 14))
plt.plot(list_k, sse, '-o')
plt.xlabel(r'Number of clusters *k*')
plt.ylabel('Sum of squared distance')
plt.show()

#%%time

# cluster the document using KMeans

# step 1 - import the model
from sklearn.cluster import KMeans, MiniBatchKMeans

# step 2 - instantiate the model
km = KMeans(n_clusters=7, random_state=42)

# step 3 - fit the model with data
# clustering is unsupervised so we do not have labels to add during .fit()
km.fit(titles_vect)

# step 4 - predict the cluster of each section_title
df['clusters'] = km.predict(titles_vect)

def review_clusters(df_small, n_clusters):
    for cl_num in range(n_clusters):
        print(cl_num)
        print(df[df.clusters == cl_num]['Review Title'].values[0:3])
        print()

review_clusters(df, n_clusters=7)

"""###Clustering Reviews"""

#Sentence Clustering: K-Means

from sklearn.feature_extraction.text import TfidfVectorizer

#Sparse Matrix
vect = TfidfVectorizer(norm='l2', stop_words='english', max_features=1000, ngram_range=(1,2))
#should we alter some of these parameters?

titles_vect = vect.fit_transform(df['Review'])
titles_vect.shape

from sklearn.cluster import KMeans, MiniBatchKMeans
import matplotlib.pyplot as plt

# Run the Kmeans algorithm, printing out the SSE to identify the inflection "elbow"
sse = []
list_k = list(range(1, 11))

for k in list_k:
    km = MiniBatchKMeans(n_clusters=k, random_state=3) # use for larger datasets, not as accurate
    #km = KMeans(n_clusters=k)
    km.fit(titles_vect)
    sse.append(km.inertia_)

# Plot sse against k
plt.figure(figsize=(14, 14))
plt.plot(list_k, sse, '-o')
plt.xlabel(r'Number of clusters *k*')
plt.ylabel('Sum of squared distance')
plt.show()

#%%time

# cluster the document using KMeans

# step 1 - import the model
from sklearn.cluster import KMeans, MiniBatchKMeans

# step 2 - instantiate the model
km = KMeans(n_clusters=5, random_state=42)

# step 3 - fit the model with data
# clustering is unsupervised so we do not have labels to add during .fit()
km.fit(titles_vect)

# step 4 - predict the cluster of each section_title
df['clusters'] = km.predict(titles_vect)

"""Notes about Review clustering:

-Don't seemed to be clustered by good or bad reviews.

-Also not clustered by features; some reviews are very long and cover lots of different topics.

-Maybe we don't have enough reviews do this is correctly.
"""

def review_clusters(df, n_clusters):
    for cl_num in range(n_clusters):
        print(cl_num)
        print(df[df.clusters == cl_num]['Review'].values[0:3])
        print()

review_clusters(df, n_clusters=5)

"""###Sentiment Analysis based on Review"""

!pip install vaderSentiment

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
analyzer = SentimentIntensityAnalyzer()

# create a function to pass our sentences
def sentiment_analyzer_scores(sentence):
    score = analyzer.polarity_scores(sentence)
    #print("{:-<60} {}".format(sentence, str(score)))
    return score

df['score'] = df['Review'].apply(lambda review: sentiment_analyzer_scores(review)['compound'])
df.head(5)

df['sentiment_label'] = df['score'].apply(lambda c: 'very pos' if c >= 0.50 else 
                                                   ('pos' if c > 0.05 else 
                                                   ('neu' if c > -0.05 else 
                                                   ('neg' if c > -0.50 else 'very neg'))))

#value count

verypos = len(df[df['sentiment_label'] == 'very pos'])
pos = len(df[df['sentiment_label'] == 'pos'])
neu = len(df[df['sentiment_label'] == 'neu'])
neg = len(df[df['sentiment_label'] == 'neg'])
veryneg = len(df[df['sentiment_label'] == 'very neg'])
print('verypos:', verypos)
print('pos:', pos)
print('neu:', neu)
print('neg:', neg)
print('veryneg:', veryneg)

"""### different distribution than actual user rating - this could be due to many factors: inaccuracy of the tweet sentiment tool, small sample size used to do the analysis, most importantly, rating by each person is very subjective"""

import numpy as np
import matplotlib.pyplot as plt

performlist = [veryneg, neg, neu, pos, verypos]

objects = ('very negative', 'negative', 'neutral', 'positive', 'very positive')
y_pos = np.arange(len(objects))
performance = performlist

plt.bar(y_pos, performance, align='center', alpha=0.75)
plt.xticks(y_pos, objects)
plt.ylabel('Reviews')
plt.title('Review Sentiment')

plt.show()